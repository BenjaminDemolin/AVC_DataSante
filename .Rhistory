round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité)
modele_compare
model_list <- list(logistic = model_glm_tuned,
knn=model_knn_tuned,
rf = model_rf_tuned,
svm = model_svm_tuned,
tree = model_tree_tuned)
results <- resamples(model_list)
summary(results)
bwplot(results)
bwplot(results, metric = "ROC")
bwplot(results, metric = "Spec")
bwplot(results, metric = "sens")
results$metrics
model_list <- list(logistic = model_glm_tuned,
knn=model_knn_tuned,
rf = model_rf_tuned,
svm = model_svm_tuned,
tree = model_tree_tuned)
results <- resamples(model_list)
summary(results)
bwplot(results)
bwplot(results, metric = "ROC")
bwplot(results, metric = "Spec")
bwplot(results, metric = "Sens")
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
prediction_glm_tuned
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
prediction_glm_tuned
pr_glm = prediction(list(prediction_glm_tuned), df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
prediction_glm_tuned
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
str(prediction_glm_tuned)
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
str(prediction_glm_tuned)
df_testing_over$stroke
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
prf_glm = performance(prediction_glm_tuned, measure = "tpr", x.measure = "fpr")
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(prediction_glm_tuned, df_testing_over$stroke)
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over, type="response")
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over, type="prob")
# matrice de confusion
matconfus_glm_tuned <- table(prediction_glm_tuned, df_testing_over$stroke)
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over, type="response")
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over, type="prob")
# matrice de confusion
matconfus_glm_tuned <- table(prediction_glm_tuned, df_testing_over$stroke)
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over, type="prob")
str(prediction_glm_tuned)
str(df_testing_over$stroke)
# matrice de confusion
matconfus_glm_tuned <- table(prediction_glm_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
prediction_glm_tuned_roc <- predict(model_glm_tuned, df_testing_over, type="prob")
# glm
pr_glm = prediction(prediction_glm_tuned_roc, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
prediction_glm_tuned_roc <- predict(model_glm_tuned, df_testing_over, type="response")
### les courbes ROC des 5 méthodes
prediction_glm_tuned_roc <- predict(model_glm_tuned, df_testing_over, type="raw")
# glm
pr_glm = prediction(prediction_glm_tuned_roc, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# knn
pr_knn = prediction(prediction_knn_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training_over,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
preProcess = c("scale", "center"),
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing_over)
# matrice de confusion
matconfus_glm_tuned <- table(prediction_glm_tuned, df_testing_over$stroke)
matconfus_glm_tuned
# calcul de l'erreur de prediction
err_glm_tuned <- 1-(sum(diag(matconfus_glm_tuned)) / sum(matconfus_glm_tuned))
err_glm_tuned*100
# kappa / sensibilité / specificté
kappa_glm_tuned <- CohenKappa(matconfus_glm_tuned)
kappa_glm_tuned
sensitivity_glm_tuned <- sensitivity(prediction_glm_tuned, df_testing_over$stroke)
sensitivity_glm_tuned
specificity_glm_tuned <- specificity(prediction_glm_tuned, df_testing_over$stroke)
specificity_glm_tuned
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(prediction_knn_tuned, df_testing_over$stroke)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les AUC pour les 5 methodes
# glm
auc_glm = performance(pr_glm, measure = "auc")
auc_glm = auc_glm@y.values[[1]]
print(paste("AUC GLM", auc_glm))
# knn
auc_knn = performance(pr_knn, measure = "auc")
auc_knn = auc_knn@y.values[[1]]
print(paste("AUC KNN", auc_knn))
# tree
auc_tree = performance(pr_tree, measure = "auc")
auc_tree = auc_tree@y.values[[1]]
print(paste("AUC tree", auc_tree))
# rf
auc_rf = performance(pr_rf, measure = "auc")
auc_rf = auc_rf@y.values[[1]]
print(paste("AUC rf", auc_rf))
# svm
auc_svm = performance(pr_svm, measure = "auc")
auc_svm = auc_svm@y.values[[1]]
print(paste("AUC svm", auc_svm)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les AUC pour les 5 methodes
# glm
auc_glm = performance(pr_glm, measure = "auc")
auc_glm = auc_glm@y.values[[1]]
print(paste("AUC GLM", auc_glm))
# knn
auc_knn = performance(pr_knn, measure = "auc")
auc_knn = auc_knn@y.values[[1]]
print(paste("AUC KNN", auc_knn))
# tree
auc_tree = performance(pr_tree, measure = "auc")
auc_tree = auc_tree@y.values[[1]]
print(paste("AUC tree", auc_tree))
# rf
auc_rf = performance(pr_rf, measure = "auc")
auc_rf = auc_rf@y.values[[1]]
print(paste("AUC rf", auc_rf))
# svm
auc_svm = performance(pr_svm, measure = "auc")
auc_svm = auc_svm@y.values[[1]]
print(paste("AUC svm", auc_svm))
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
round("x",2),
round(auc_svm,2),
round("x",2),
round(auc_tree,2),
round("x",2),
round(auc_rf,2)
)
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
"x",
round(auc_svm,2),
"x",
round(auc_tree,2),
"x",
round(auc_rf,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité)
modele_compare
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
"x",
round(auc_svm,2),
"x",
round(auc_tree,2),
"x",
round(auc_rf,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité, AUC)
modele_compare
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les AUC pour les 5 methodes
# glm
auc_glm = performance(pr_glm, measure = "auc")
auc_glm = auc_glm@y.values[[1]]
# knn
auc_knn = performance(pr_knn, measure = "auc")
auc_knn = auc_knn@y.values[[1]]
# tree
auc_tree = performance(pr_tree, measure = "auc")
auc_tree = auc_tree@y.values[[1]]
# rf
auc_rf = performance(pr_rf, measure = "auc")
auc_rf = auc_rf@y.values[[1]]
# svm
auc_svm = performance(pr_svm, measure = "auc")
auc_svm = auc_svm@y.values[[1]]
