urbain_avc <- (type_residence$Fréquence[type_residence$Type_residence =="Urban" & type_residence$AVC == "avcP"] / type_residence$Fréquence[type_residence$Type_residence =="Urban" & type_residence$AVC == "avcN"])*100
print(paste("Pourcentage de rural AVC : " ,round(rural_avc,2),"%"))
print(paste("Pourcentage de urbain AVC : " ,round(urbain_avc),"%"))
# plot hist Résidence
ggplot(type_residence, aes(x = Type_residence, y = Fréquence, fill = AVC)) +
geom_bar(stat = "identity") +
ggtitle("Type de résidence en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))+
geom_text(aes(label = Fréquence), vjust = 0)
# Variable - Niveau glucose
min(avc0$avg_glucose_level)
max(avc0$avg_glucose_level)
min(avc1$avg_glucose_level)
max(avc1$avg_glucose_level)
# Plot hist avg_glucose_level avc = 0
ggplot(avc0, aes(x=avg_glucose_level)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2,col="#00AFBB", fill="#00AFBB", breaks=seq(55, 275, by = 10) )+
geom_density(col = 2) +
ggtitle("Histogramme des niveaux de glucose des non AVCs") + theme(plot.title = element_text(hjust = 0.5))
# Plot hist avg_glucose_level avc = 1
ggplot(avc1, aes(x=avg_glucose_level)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2,col="#00AFBB", fill="#00AFBB", breaks=seq(55, 275, by = 10) )+
geom_density(col = 2) +
ggtitle("Histogramme des niveaux de glucose des AVCs") + theme(plot.title = element_text(hjust = 0.5))
# Plot hist et densité avg_glucose_level avc
ggplot(data_format, aes(x=avg_glucose_level, color =stroke, fill = stroke)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2, breaks=seq(55, 275, by = 10) )+
geom_density(alpha=0.2) +
ggtitle("Histogramme des niveaux de glucose en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# boxPlot avg_glucose_level
ggplot(data_format, aes(x=stroke, y=avg_glucose_level, color=stroke)) +
geom_boxplot() +
ggtitle("Boxplot des niveaux de glucose en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# Regroupement selon les niveaux de glucose
summary(data_format$avg_glucose_level)
data_format$glucoseQ <- cut(data_format$avg_glucose_level,
breaks = c(-Inf
,69, 99,125
, Inf),
labels = c("inf_69", "70_100"
,"101_126","127_inf"
),
right = FALSE)
stroke_glc <- as.data.frame(table(data_format$stroke, data_format$glucoseQ))
stroke_glc
# conversion de glucoseQ chr en num
data_format$glucoseQ <- suppressWarnings(as.factor(data_format$glucoseQ))
# dataframe de classe glucose en fonction d'avc
glucoseQ <- as.data.frame(table(data_format$glucoseQ, data_format$stroke))
colnames(glucoseQ) <- c('Classe_glucose','AVC','Fréquence')
ggplot(glucoseQ, aes(x = Classe_glucose, y = Fréquence, fill = AVC)) +
geom_bar(stat = "identity") +
ggtitle("Classe de niveau de glucose en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))+
geom_text(aes(label = Fréquence), vjust = 0)
# Variable - IMC
min(avc0$bmi)
max(avc0$bmi)
min(avc1$bmi)
max(avc1$bmi)
# Plot hist IMC avc = 0
ggplot(avc0, aes(x=bmi)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2,col="#00AFBB", fill="#00AFBB", breaks=seq(10, 100, by = 5) )+
geom_density(col = 2) +
ggtitle("Histogramme des IMC des non AVCs") + theme(plot.title = element_text(hjust = 0.5))
# Plot hist IMC avc = 1
ggplot(avc1, aes(x=bmi)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2,col="#00AFBB", fill="#00AFBB", breaks=seq(10, 100, by = 5) )+
geom_density(col = 2) +
ggtitle("Histogramme des IMC des AVCs") + theme(plot.title = element_text(hjust = 0.5))
# Plot hist et densité IMC avc
ggplot(data_format, aes(x=bmi, color =stroke, fill = stroke)) +
geom_histogram(aes(y =..density..),position="identity", alpha=0.2, breaks=seq(10, 100, by = 5) )+
geom_density(alpha=0.2) +
ggtitle("Histogramme des IMC en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# boxPlot IMC
ggplot(data_format, aes(x=stroke, y=bmi, color=stroke)) +
geom_boxplot() +
ggtitle("Boxplot des IMC en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# Regroupement selon les groupes d'IMC
summary(data_format$bmi)
data_format$bmiQ <- cut(data_format$bmi,
breaks = c(-Inf
,24.9, 29.9, 34.9
, Inf),
labels = c("inf_25"
,"25_30","30_35",
"35_inf"
),
right = FALSE)
stroke_bmi <- as.data.frame(table(data_format$stroke, data_format$bmiQ))
stroke_bmi
# conversion de bmiQ chr en num
data_format$bmiQ <- suppressWarnings(as.factor(data_format$bmiQ))
data_format$bmiQ
# dataframe de classe imc en fonction d'avc
bmiQ <- as.data.frame(table(data_format$bmiQ, data_format$stroke))
colnames(bmiQ) <- c('Classe_IMC','AVC','Fréquence')
head(bmiQ)
ggplot(bmiQ, aes(x = Classe_IMC, y = Fréquence, fill = AVC)) +
geom_bar(stat = "identity") +
ggtitle("Classe de niveau d'IMC en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))+
geom_text(aes(label = Fréquence), vjust = 0)
# Variable - Fumeur
# dataframe de Fumeur en fonction d'avc
fumeur <- as.data.frame(table(data_format$smoking_status, data_format$stroke))
colnames(fumeur) <- c('Statut_fumeur','AVC','Fréquence')
head(fumeur)
# calcul pourcentage avc par Fumeur
ancien_fumeur_avc <- (fumeur$Fréquence[fumeur$Statut_fumeur =="formerly" & fumeur$AVC == "avcP"] / fumeur$Fréquence[fumeur$Statut_fumeur =="formerly" & fumeur$AVC == "avcN"])*100
non_fumeur_avc <- (fumeur$Fréquence[fumeur$Statut_fumeur =="never" & fumeur$AVC == "avcP"] / fumeur$Fréquence[fumeur$Statut_fumeur =="never" & fumeur$AVC == "avcN"])*100
fumeur_avc <- (fumeur$Fréquence[fumeur$Statut_fumeur =="smokes" & fumeur$AVC == "avcP"] / fumeur$Fréquence[fumeur$Statut_fumeur =="smokes" & fumeur$AVC == "avcN"])*100
print(paste("Pourcentage de ancien fumeur AVC : " ,round(ancien_fumeur_avc,2),"%"))
print(paste("Pourcentage de non fumeur AVC : " ,round(non_fumeur_avc,2),"%"))
print(paste("Pourcentage de fumeur AVC : " ,round(fumeur_avc,2),"%"))
# plot hist Fumeur
ggplot(fumeur, aes(x = Statut_fumeur, y = Fréquence, fill = AVC)) +
geom_bar(stat = "identity") +
ggtitle("Statut fumeur en fonction des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))+
geom_text(aes(label = Fréquence), vjust = 0)
# Variable - AVC
# dataframe de AVC
avc <- as.data.frame(table( data_format$stroke))
colnames(avc) <- c('AVC','Fréquence')
head(avc)
# plot hist genre
ggplot(avc, aes(x = AVC, y = Fréquence, fill = AVC)) +
geom_bar(stat = "identity") +
ggtitle("Nombre d'AVC du jeu de donnée") + theme(plot.title = element_text(hjust = 0.5), legend.position="none")+
geom_text(aes(label = Fréquence), vjust = 0)
# garde attr quanti
data_cor<- select(data_format,age,avg_glucose_level,bmi)
# calcul corrélation
M<-cor(data_cor)
# plot corrélation
corrplot(M, method="circle")
corrplot(M, method="color")
corrplot(M, method="number")
# plot IMC en fonction de age
ggplot(data_format, aes(x = age, y = bmi, colour = stroke)) +
geom_point(alpha=0.5)+
ggtitle("IMC en fonction de l'âge et des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# plot niveau de glucose en fonction de age
ggplot(data_format, aes(x = age, y = avg_glucose_level, colour = stroke)) +
geom_point(alpha=0.5)+
ggtitle("Niveau de glucose en fonction de l'âge et des chances de faire un AVC") + theme(plot.title = element_text(hjust = 0.5))
# avec age
var.test(age~stroke,data=data_format) # sous reserve d'hypothese de normalite des distributions des ages selon absence/presence d'AVC
t.test(age~stroke,var.equal=T,data=data_format)
# difference signifcative ### => le risque augmente avec l'age
# avec NivGluc
var.test(avg_glucose_level~stroke,data=data_format)
t.test(avg_glucose_level~stroke,var.equal=T,data=data_format)
# difference signifcative ####  => le risque augmente avec le niveau de glucose
# avec IMC
var.test(bmi~stroke,data=data_format)
t.test(bmi~stroke,var.equal=T,data=data_format)
# difference non signifcative (discutable)  => le risque n'augmente pas significativement avec l'IMC
# Genre
t <- chisq.test(data_format$gender,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals #===> les hommes ont plus de chance de faire un AVC que les femmes
# Maladie cardiaque
t <- chisq.test(data_format$heart_disease,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # les maladies cardiaques ont plus de chance d'etre associees avec la survenue d'un AVC
# Mariage
t <- chisq.test(data_format$ever_married,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # => les personnes ayant deja ete mariees ont plus de chance de faire un AVC
# Travail
t <- chisq.test(data_format$work_type,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # => les personnes self_employed ont plus de chance de faire un AVC
# Residence
t <- chisq.test(data_format$Residence_type,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals #==> les personnes vivant en milieu urbain ont plus de chance de faire un AVC
# Fumeur
t <- chisq.test(data_format$smoking_status,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # les personnes ayant deja fume ont plus de chance de faire un AVC
# age
t <- chisq.test(data_format$ageQ,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # les personnes de plus de 65 ans ont plus de chance de faire un AVC
# glucose
t <- chisq.test(data_format$glucoseQ,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # les personnes ayant un niveau de glucose supérieur a 127 ont plus de chance de faire un AVC
# bmi
t <- chisq.test(data_format$bmiQ,data_format$stroke) #rejet de l'hypothese d'independance
t$residuals # les personnes ayant un imc entre 30 et 35 ont plus de chance de faire un AVC
# selection des seules variables qualitatives pour l'AFCM
avcQ = data_format[,-c(2,8,9)]
summary(avcQ)
quali <- avcQ[,-8]
var_sup <- avcQ[,"stroke"]
acm <- dudi.acm(quali, scannf = FALSE, nf = 5)
## Variable supplmentaire : variable cible Coro
acm$supv <- supcol(acm, dudi.acm(var_sup, scannf = FALSE, nf = 5)$tab)
#explor(acm)
# creation de la variable interaction entre Sexe et Age
ageHeart=avcQ$ageQ:avcQ$heart_disease
table(ageHeart,avcQ[,"stroke"])
# test de dependence avec la variable cible
t <- chisq.test(ageHeart,avcQ$stroke)
t$residuals
#surtout l'age qui a un impact
avcQ2=data.frame(avcQ,"ageHeart"=ageHeart)
summary(avcQ2)
quali2 <- avcQ2[,-c(1,9,10)]
var_sup2 <- avcQ2[,c("ageQ","stroke","heart_disease")]
acm2 <- dudi.acm(quali2, scannf = FALSE, nf = 5)
acm2$supv <- supcol(acm2, dudi.acm(var_sup2, scannf = FALSE, nf = 5)$tab)
#explor(acm2)
imbalanceRatio(as.data.frame(data_format), classAttr = "stroke")
stroke_test <- data_format %>%
mutate(
stroke = as.character(stroke),
across(where(is.factor), as.numeric),
stroke = factor(stroke)
)
stroke_oversampled <- oversample(as.data.frame(stroke_test), classAttr = "stroke", ratio = 1, method = "MWMOTE")
str(stroke_oversampled)
stroke_oversampled %>%
group_by(stroke) %>%
summarize(n = n()) %>%
mutate(prop = round(n / sum(n), 2))
for ( i in 1:ncol(data_format)) {
if(is.factor(data_format[,i])){
stroke_oversampled[,i] <- factor(as.integer(stroke_oversampled[,i]),labels=c(levels(data_format[,i])))
}
}
str(stroke_oversampled)
### APPRENTISSAGE SUPERVISE
# objectif double :
## Prédire efficacement la présence (ou l’absence) d’un AVC (variable ‘stroke’)
## à partir de nouvelles observations sur les variables explicatives
## Comprendre les facteurs influençant la présence d’une maladie coronarienne
data_format_raw <- stroke_oversampled[-c(12,13,14)]
# separation echantillons apprentissage / test
set.seed(42)
df_sampling_index <- createDataPartition(data_format_raw$stroke, times = 1, p = 0.7, list = FALSE)
df_training <- data_format_raw[df_sampling_index, ]
df_testing <-  data_format_raw[-df_sampling_index, ]
prop.table(table(df_training$stroke))
prop.table(table(df_testing$stroke))
### PHASE D APRENTISSAGE
## 1) On lance les algorithmes avec les parametres par defaut
## 2) On optimise les parametres de complexite par CV V-5
### on definit le cadre general de l'optimisation des parametres de complexite dans la fonction trainControl (a utiliser dans la fonciton train du package caret)
df_control <- trainControl(method="cv", #validation croisee
number = 5,  # 5 folds (selon le nombre de donnees on peut mettre plus)
classProbs = TRUE,
summaryFunction = twoClassSummary) # on est dans un cas de classification binaire
# apprentissage
model_glm <- glm(stroke ~., data=df_training, family=binomial(link="logit"))
summary(model_glm)
# prediction
prediction_glm <- predict(model_glm, df_testing, type="response")
prediction_glm <- ifelse(prediction_glm<0.5,"avcN_pred","avcP_pred")
# matrice de confusion
matconfus_glm <- table(prediction_glm, df_testing$stroke)
matconfus_glm
# calcul de l'erreur de prediction
err_glm <- 1-(sum(diag(matconfus_glm)) / sum(matconfus_glm))
err_glm*100
# kappa
CohenKappa(matconfus_glm)
# apprentissage et tuning
set.seed(42)
model_glm_tuned = train(stroke ~.,
df_training,
method = "glmStepAIC", ## selection forward par minimisation de l'AIC
metric="ROC",
trControl = df_control)
summary(model_glm_tuned$finalModel)
# prediction
prediction_glm_tuned <- predict(model_glm_tuned, df_testing)
# matrice de confusion
matconfus_glm_tuned <- table(prediction_glm_tuned, df_testing$stroke)
matconfus_glm_tuned
# calcul de l'erreur de prediction
err_glm_tuned <- 1-(sum(diag(matconfus_glm_tuned)) / sum(matconfus_glm_tuned))
err_glm_tuned*100
# kappa
CohenKappa(matconfus_glm_tuned)
# apprentissage et tuning
set.seed(42)
grid <- expand.grid(.k=seq(1,50,by=1))
model_knn_tuned <- train(stroke ~.,
data = df_training,
method = "knn",
metric = "ROC",
preProcess = c('center', 'scale'),
tuneGrid= grid,
trControl = df_control)
# affichage du modele
model_knn_tuned$finalModel
k = model_knn_tuned$results$k[model_knn_tuned$results$ROC == max(model_knn_tuned$results$ROC)]
ggplot(model_knn_tuned)+geom_vline(xintercept = k,color ='red') # performance metric (ROC) en fonction des valeurs de C (le parametre a optimiser)
# prediction
prediction_knn_tuned <- predict(model_knn_tuned, df_testing,type="prob")
prediction_knn_tuned <- predict(model_knn_tuned, df_testing)#,type="prob")
# matrice de confusion
matconfus_knn_tuned <- table(prediction_knn_tuned, df_testing$stroke)
matconfus_knn_tuned
# calcul de l'erreur de prediction
err_knn_tuned <- 1-(sum(diag(matconfus_knn_tuned)) / sum(matconfus_knn_tuned))
err_knn_tuned*100
# kappa
CohenKappa(matconfus_knn_tuned)
k
# apprentissage
model_svm=svm(stroke~., data=df_training, cost=0.5, kernel="linear")
# prediction
prediction_svm <- predict(model_svm, df_testing)
# matrice de confusion
matconfus_svm <- table(prediction_svm, df_testing$stroke)
matconfus_svm
# calcul de l'erreur de prediction
err_svm <- 1-(sum(diag(matconfus_svm)) / sum(matconfus_svm))
err_svm*100
# kappa
CohenKappa(matconfus_svm)
# apprentissage et tuning
grid <- expand.grid(C=seq(0.01,1,0.03))
set.seed(42)
model_svm_tuned <- train(stroke ~., data = df_training,
method = "svmLinear",
metric = "ROC",
preProcess = c("scale", "center"),
tuneGrid= grid,
trControl = df_control)
# affichage du modele
model_svm_tuned$finalModel
c = model_svm_tuned$results$C[model_svm_tuned$results$ROC == max(model_svm_tuned$results$ROC)]
ggplot(model_svm_tuned)+geom_vline(xintercept = c,color ='red')
# prediction
prediction_svm_tuned <- predict(model_svm_tuned, df_testing)
# matrice de confusion
matconfus_svm_tuned <- table(prediction_svm_tuned, df_testing$stroke)
matconfus_svm_tuned
# calcul de l'erreur de prediction
err_svm_tuned <- 1-(sum(diag(matconfus_svm_tuned)) / sum(matconfus_svm_tuned))
err_svm_tuned*100
# kappa
CohenKappa(matconfus_svm_tuned)
c
# apprentissage
tree <- rpart(stroke ~.,data=df_training, method="class",
control=rpart.control(minsplit=1,cp=0,xval=10))
# affichage du modele
plot(tree)
text(tree, use.n.=TRUE, all=TRUE, cex=.8)
# prediction
prediction_tree <- predict(tree, df_testing, type="class")
# matrice de confusion
matconfus_tree <- table(prediction_tree, df_testing$stroke)
matconfus_tree
# calcul de l'erreur de prediction
err_tree <- 1-(sum(diag(matconfus_tree)) / sum(matconfus_tree))
err_tree*100
# kappa
CohenKappa(matconfus_tree)
# apprentissage et tuning
set.seed(42)
grid <- expand.grid(cp=seq(0.01,1,0.01))
model_tree_tuned = train(stroke ~.,
data=df_training,
method="rpart",
metric = "ROC",
tuneGrid= grid,
trControl = df_control)
# affichage du modele
model_tree_tuned$finalModel
summary(model_tree_tuned$finalModel)
plot(model_tree_tuned$finalModel, uniform=TRUE, main="Pruned Classification Tree")
text(model_tree_tuned$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
# prediction
prediction_tree_tuned <- predict(model_tree_tuned,  df_testing)
# matrice de confusion
matconfus_tree_tuned <- table(prediction_tree_tuned, df_testing$stroke)
matconfus_tree_tuned
# calcul de l'erreur de prediction
err_tree_tuned <- 1-(sum(diag(matconfus_tree_tuned)) / sum(matconfus_tree_tuned))
err_tree_tuned*100
# kappa
CohenKappa(matconfus_tree_tuned)
# apprentissage
set.seed(42)
model_rf <- train(stroke ~.,
data = df_training,
method = "rf",
metric = "ROC",
trControl = df_control)
model_rf$finalModel
# prediction
prediction_rf <- predict(model_rf, df_testing)
# matrice de confusion
matconfus_rf <- table(prediction_rf, df_testing$stroke)
matconfus_rf
# calcul de l'erreur de prediction
err_rf <- 1-(sum(diag(matconfus_rf)) / sum(matconfus_rf))
err_rf*100
# kappa
CohenKappa(matconfus_rf)
# apprentissage et tuning
grid <- expand.grid(mtry=2:12)
set.seed(42)
model_rf_tuned <- train(stroke ~.,
data = df_training,
method = "rf",
metric = "ROC",
tuneGrid= grid,
trControl = df_control)
# affichage du modele
plot(model_rf_tuned)
plot(model_rf_tuned$finalModel)
model_rf_tuned$finalModel
varImpPlot(model_rf_tuned$finalModel, sort = TRUE, n.var = 13, main = "Variables importance")
# prediction
prediction_rf_tuned <- predict(model_rf_tuned, df_testing)
# matrice de confusion
matconfus_rf_tuned <- table(prediction_rf_tuned, df_testing$stroke)
matconfus_rf_tuned
# calcul de l'erreur de prediction
err_rf_tuned <- 1-(sum(diag(matconfus_rf_tuned)) / sum(matconfus_rf_tuned))
err_rf_tuned*100
# kappa
CohenKappa(matconfus_rf_tuned)
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(CohenKappa(matconfus_glm),2),
round(CohenKappa(matconfus_glm_tuned),2),
round(CohenKappa(matconfus_knn_tuned),2),
round(CohenKappa(matconfus_svm),2),
round(CohenKappa(matconfus_svm_tuned),2),
round(CohenKappa(matconfus_tree),2),
round(CohenKappa(matconfus_tree_tuned),2),
round(CohenKappa(matconfus_rf),2),
round(CohenKappa(matconfus_rf_tuned),2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa)
modele_compare
model_list <- list(logistic = model_glm_tuned,
knn=model_knn_tuned,
rf = model_rf_tuned,
svm = model_svm_tuned,
tree = model_tree_tuned)
results <- resamples(model_list)
summary(results)
bwplot(results, metric = "ROC")
df_pred <- data.frame(obs=as.numeric(df_testing$stroke)-1,
glm=as.numeric(prediction_glm_tuned)-1,
knn=as.numeric(prediction_knn_tuned)-1,
tree=as.numeric(prediction_tree_tuned)-1,
rf=as.numeric(prediction_rf_tuned)-1,
svm=as.numeric(prediction_svm_tuned)-1)
plot(roc(df_pred$obs,df_pred$glm))
plot(roc(df_pred$obs,df_pred$knn),add=TRUE,col="red")
plot(roc(df_pred$obs,df_pred$tree),add=TRUE,col="purple")
plot(roc(df_pred$obs,df_pred$rf),add=TRUE,col="green")
plot(roc(df_pred$obs,df_pred$svm),add=TRUE,col="blue")
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
df_pred <- data.frame(obs=as.numeric(df_testing$stroke)-1,
glm=as.numeric(prediction_glm_tuned)-1,
knn=as.numeric(prediction_knn_tuned)-1,
tree=as.numeric(prediction_tree_tuned)-1,
rf=as.numeric(prediction_rf_tuned)-1,
svm=as.numeric(prediction_svm_tuned)-1)
plot(roc(df_pred$obs,df_pred$glm))
plot(roc(df_pred$obs,df_pred$knn),add=TRUE,col="red")
plot(roc(df_pred$obs,df_pred$tree),add=TRUE,col="purple")
plot(roc(df_pred$obs,df_pred$rf),add=TRUE,col="green")
plot(roc(df_pred$obs,df_pred$svm),add=TRUE,col="blue")
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
df_pred <- data.frame(obs=as.numeric(df_testing$stroke)-1,
glm=as.numeric(prediction_glm_tuned)-1,
knn=as.numeric(prediction_knn_tuned)-1,
tree=as.numeric(prediction_tree_tuned)-1,
rf=as.numeric(prediction_rf_tuned)-1,
svm=as.numeric(prediction_svm_tuned)-1)
supressWarnings(plot(roc(df_pred$obs,df_pred$glm)))
df_pred <- data.frame(obs=as.numeric(df_testing$stroke)-1,
glm=as.numeric(prediction_glm_tuned)-1,
knn=as.numeric(prediction_knn_tuned)-1,
tree=as.numeric(prediction_tree_tuned)-1,
rf=as.numeric(prediction_rf_tuned)-1,
svm=as.numeric(prediction_svm_tuned)-1)
suppressWarnings(plot(roc(df_pred$obs,df_pred$glm)))
plot(roc(df_pred$obs,df_pred$knn),add=TRUE,col="red")
plot(roc(df_pred$obs,df_pred$tree),add=TRUE,col="purple")
plot(roc(df_pred$obs,df_pred$rf),add=TRUE,col="green")
plot(roc(df_pred$obs,df_pred$svm),add=TRUE,col="blue")
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
df_pred <- data.frame(obs=as.numeric(df_testing$stroke)-1,
glm=as.numeric(prediction_glm_tuned)-1,
knn=as.numeric(prediction_knn_tuned)-1,
tree=as.numeric(prediction_tree_tuned)-1,
rf=as.numeric(prediction_rf_tuned)-1,
svm=as.numeric(prediction_svm_tuned)-1)
plot(roc(df_pred$obs,df_pred$glm))
plot(roc(df_pred$obs,df_pred$knn),add=TRUE,col="red")
plot(roc(df_pred$obs,df_pred$tree),add=TRUE,col="purple")
plot(roc(df_pred$obs,df_pred$rf),add=TRUE,col="green")
plot(roc(df_pred$obs,df_pred$svm),add=TRUE,col="blue")
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
