set.seed(42)
model_rf <- train(stroke ~.,
data = df_training_over,
method = "rf",
metric = "ROC",
preProcess = c("scale", "center"),
trControl = df_control)
model_rf$finalModel
# prediction
prediction_rf <- predict(model_rf, df_testing_over)
# matrice de confusion
matconfus_rf <- table(prediction_rf, df_testing_over$stroke)
matconfus_rf
# calcul de l'erreur de prediction
err_rf <- 1-(sum(diag(matconfus_rf)) / sum(matconfus_rf))
err_rf*100
# kappa / sensibilité / specificté
kappa_rf <- CohenKappa(matconfus_rf)
kappa_rf
sensitivity_rf <- sensitivity(prediction_rf, df_testing_over$stroke)
sensitivity_rf
specificity_rf <- specificity(prediction_rf, df_testing_over$stroke)
specificity_rf
# apprentissage et tuning
set.seed(42)
grid <- expand.grid(mtry=2:10)
model_rf_tuned <- train(stroke ~.,
data = df_training_over,
method = "rf",
metric = "Accuracy",
tuneGrid= grid,
preProcess = c("scale", "center"),
trControl = df_control)
# affichage du modele
plot(model_rf_tuned)
plot(model_rf_tuned$finalModel)
model_rf_tuned$finalModel
varImpPlot(model_rf_tuned$finalModel, sort = TRUE, n.var = 13, main = "Variables importance")
# prediction
prediction_rf_tuned <- predict(model_rf_tuned, df_testing_over)
# matrice de confusion
matconfus_rf_tuned <- table(prediction_rf_tuned, df_testing_over$stroke)
matconfus_rf_tuned
# calcul de l'erreur de prediction
err_rf_tuned <- 1-(sum(diag(matconfus_rf_tuned)) / sum(matconfus_rf_tuned))
err_rf_tuned*100
# kappa / sensibilité / specificté
kappa_rf_tuned <- CohenKappa(matconfus_rf_tuned)
kappa_rf_tuned
sensitivity_rf_tuned <- sensitivity(prediction_rf_tuned, df_testing_over$stroke)
sensitivity_rf_tuned
specificity_rf_tuned <- specificity(prediction_rf_tuned, df_testing_over$stroke)
specificity_rf_tuned
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les AUC pour les 5 methodes
# glm
auc_glm = performance(pr_glm, measure = "auc")
auc_glm = auc_glm@y.values[[1]]
# knn
auc_knn = performance(pr_knn, measure = "auc")
auc_knn = auc_knn@y.values[[1]]
# tree
auc_tree = performance(pr_tree, measure = "auc")
auc_tree = auc_tree@y.values[[1]]
# rf
auc_rf = performance(pr_rf, measure = "auc")
auc_rf = auc_rf@y.values[[1]]
# svm
auc_svm = performance(pr_svm, measure = "auc")
auc_svm = auc_svm@y.values[[1]]
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
"x",
round(auc_svm,2),
"x",
round(auc_tree,2),
"x",
round(auc_rf,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité, AUC)
modele_compare
write.csv(modele_compare,"./modele_compare.csv", row.names = FALSE)
model_list <- list(logistic = model_glm_tuned,
knn=model_knn_tuned,
rf = model_rf_tuned,
svm = model_svm_tuned,
tree = model_tree_tuned)
results <- resamples(model_list)
summary(results)
bwplot(results)
bwplot(results, metric = "ROC")
bwplot(results, metric = "Spec")
bwplot(results, metric = "Sens")
# apprentissage et tuning
set.seed(42)
grid <- expand.grid(mtry=2:10)
model_rf_tuned <- train(stroke ~.,
data = df_training_over,
method = "rf",
metric = "ROC",
tuneGrid= grid,
preProcess = c("scale", "center"),
trControl = df_control)
# affichage du modele
plot(model_rf_tuned)
plot(model_rf_tuned$finalModel)
model_rf_tuned$finalModel
varImpPlot(model_rf_tuned$finalModel, sort = TRUE, n.var = 13, main = "Variables importance")
# prediction
prediction_rf_tuned <- predict(model_rf_tuned, df_testing_over)
# matrice de confusion
matconfus_rf_tuned <- table(prediction_rf_tuned, df_testing_over$stroke)
matconfus_rf_tuned
# calcul de l'erreur de prediction
err_rf_tuned <- 1-(sum(diag(matconfus_rf_tuned)) / sum(matconfus_rf_tuned))
err_rf_tuned*100
# kappa / sensibilité / specificté
kappa_rf_tuned <- CohenKappa(matconfus_rf_tuned)
kappa_rf_tuned
sensitivity_rf_tuned <- sensitivity(prediction_rf_tuned, df_testing_over$stroke)
sensitivity_rf_tuned
specificity_rf_tuned <- specificity(prediction_rf_tuned, df_testing_over$stroke)
specificity_rf_tuned
summary(model_glm_tuned$finalModel)
model_knn_tuned$finalModel
summary(model_knn_tuned$finalModel)
model_knn_tuned$finalModel
model_svm_tuned$finalModel
model_tree_tuned$finalModel
summary(model_tree_tuned$finalModel)
summary(model_tree_tuned$finalModel)
model_tree_tuned$finalModel
model_rf_tuned$finalModel
data_format <- data
str(data_format)
nrow(data_format)
# supprime id
data_format<- data_format[,-c(1)]
# conversion de bmi chr en num
data_format$bmi <- suppressWarnings(as.numeric(data_format$bmi))
# supprime le cas ou genre = other (une fois)
data_format<-data_format[!(data_format$gender=="Other"),]
data_format<-data_format[!(data_format$smoking_status=="Unknown"),]
# assemble deux facteur d'un attribut en un (pas assez nombreux pour deux)
data_format$work_type[data_format$work_type == "children" | data_format$work_type == "Never_worked"] <- "never_worked"
# conversion chr en facteur
data_format <- as.data.frame(unclass(data_format), stringsAsFactors = TRUE)
# supprime les na
data_format <- na.omit(data_format)
# les binaires (hypertension, heart_disease et stroke) en facteur
data_format$hypertension <- factor(as.factor(data_format$hypertension),labels=c("htN","htP"))
data_format$heart_disease <- factor(as.factor(data_format$heart_disease),labels=c("hdN","hdP"))
data_format$stroke <- factor(as.factor(data_format$stroke),labels=c("avcN","avcP"))
# change les labels qui ne sont pas utilisables en R
data_format$work_type <- factor(as.factor(data_format$work_type),labels=c("never_worked","govt_job","private","self_employed"))
data_format$smoking_status <- factor(as.factor(data_format$smoking_status),labels=c("formerly","never","smokes"))
summary(data_format)
nrow(data_format)
data_format <- data
str(data_format)
nrow(data_format)
# supprime id
data_format<- data_format[,-c(1)]
# conversion de bmi chr en num
data_format$bmi <- suppressWarnings(as.numeric(data_format$bmi))
# supprime le cas ou genre = other (une fois)
data_format<-data_format[!(data_format$gender=="Other"),]
data_format<-data_format[!(data_format$smoking_status=="Unknown"),]
# assemble deux facteur d'un attribut en un (pas assez nombreux pour deux)
data_format$work_type[data_format$work_type == "children" | data_format$work_type == "Never_worked"] <- "never_worked"
# conversion chr en facteur
data_format <- as.data.frame(unclass(data_format), stringsAsFactors = TRUE)
# supprime les na
data_format <- na.omit(data_format)
# les binaires (hypertension, heart_disease et stroke) en facteur
data_format$hypertension <- factor(as.factor(data_format$hypertension),labels=c("htN","htP"))
data_format$heart_disease <- factor(as.factor(data_format$heart_disease),labels=c("hdN","hdP"))
data_format$stroke <- factor(as.factor(data_format$stroke),labels=c("avcN","avcP"))
# change les labels qui ne sont pas utilisables en R
data_format$work_type <- factor(as.factor(data_format$work_type),labels=c("never_worked","govt_job","private","self_employed"))
data_format$smoking_status <- factor(as.factor(data_format$smoking_status),labels=c("formerly","never","smokes"))
str(data_format)
nrow(data_format)
data_format <- data
str(data_format)
nrow(data_format)
# supprime id
data_format<- data_format[,-c(1)]
# conversion de bmi chr en num
data_format$bmi <- suppressWarnings(as.numeric(data_format$bmi))
# supprime le cas ou genre = other (une fois)
data_format<-data_format[!(data_format$gender=="Other"),]
data_format<-data_format[!(data_format$smoking_status=="Unknown"),]
# assemble deux facteur d'un attribut en un (pas assez nombreux pour deux)
data_format$work_type[data_format$work_type == "children" | data_format$work_type == "Never_worked"] <- "never_worked"
# conversion chr en facteur
data_format <- as.data.frame(unclass(data_format), stringsAsFactors = TRUE)
# supprime les na
data_format <- na.omit(data_format)
# les binaires (hypertension, heart_disease et stroke) en facteur
data_format$hypertension <- factor(as.factor(data_format$hypertension),labels=c("htN","htP"))
data_format$heart_disease <- factor(as.factor(data_format$heart_disease),labels=c("hdN","hdP"))
data_format$stroke <- factor(as.factor(data_format$stroke),labels=c("avcN","avcP"))
# change les labels qui ne sont pas utilisables en R
data_format$work_type <- factor(as.factor(data_format$work_type),labels=c("never_worked","govt_job","private","self_employed"))
data_format$smoking_status <- factor(as.factor(data_format$smoking_status),labels=c("formerly","never","smokes"))
summary(data_format)
nrow(data_format)
set.seed(42)
# apprentissage et tuning
grid <- expand.grid(cp=seq(0.001,0.01,0.001))
model_tree_tuned = train(stroke ~.,
data=df_training_over,
method="rpart",
metric = "ROC",
preProcess = c("scale", "center"),
tuneGrid= grid,
trControl = df_control)
# affichage du modele
plot(model_tree_tuned$finalModel, uniform=TRUE, main="Pruned Classification Tree")
text(model_tree_tuned$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
# prediction
prediction_tree_tuned <- predict(model_tree_tuned,  df_testing_over)
# matrice de confusion
matconfus_tree_tuned <- table(prediction_tree_tuned, df_testing_over$stroke)
matconfus_tree_tuned
# calcul de l'erreur de prediction
err_tree_tuned <- 1-(sum(diag(matconfus_tree_tuned)) / sum(matconfus_tree_tuned))
err_tree_tuned*100
# kappa / sensibilité / specificté
kappa_tree_tuned <- CohenKappa(matconfus_tree_tuned)
kappa_tree_tuned
sensitivity_tree_tuned <- sensitivity(prediction_tree_tuned, df_testing_over$stroke)
sensitivity_tree_tuned
specificity_tree_tuned <- specificity(prediction_tree_tuned, df_testing_over$stroke)
specificity_tree_tuned
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
"x",
round(auc_svm,2),
"x",
round(auc_tree,2),
"x",
round(auc_rf,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité, AUC)
modele_compare
write.csv(modele_compare,"./modele_compare.csv", row.names = FALSE)
### les courbes ROC des 5 méthodes
# glm
pr_glm = prediction(as.numeric(prediction_glm_tuned), as.numeric(df_testing_over$stroke))
prf_glm = performance(pr_glm, measure = "tpr", x.measure = "fpr")
plot(prf_glm)
# knn
pr_knn = prediction(as.numeric(prediction_knn_tuned), as.numeric(df_testing_over$stroke))
prf_knn = performance(pr_knn, measure = "tpr", x.measure = "fpr")
plot(prf_knn, col="red", add=TRUE)
# tree
pr_tree = prediction(as.numeric(prediction_tree_tuned), as.numeric(df_testing_over$stroke))
prf_tree = performance(pr_tree, measure = "tpr", x.measure = "fpr")
plot(prf_tree, col="purple",add=TRUE)
# rf
pr_rf = prediction(as.numeric(prediction_rf_tuned), as.numeric(df_testing_over$stroke))
prf_rf = performance(pr_rf, measure = "tpr", x.measure = "fpr")
plot(prf_rf, col="green", add=TRUE)
# svm
pr_svm = prediction(as.numeric(prediction_svm_tuned), as.numeric(df_testing_over$stroke))
prf_svm = performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="blue", add=TRUE)
legend("bottomright", inset = 0.1, legend = c("glm", "knn", "tree", "rf", "svm"), lty = 1,
col = c("black", "red", "purple","green","blue"))
### les AUC pour les 5 methodes
# glm
auc_glm = performance(pr_glm, measure = "auc")
auc_glm = auc_glm@y.values[[1]]
# knn
auc_knn = performance(pr_knn, measure = "auc")
auc_knn = auc_knn@y.values[[1]]
# tree
auc_tree = performance(pr_tree, measure = "auc")
auc_tree = auc_tree@y.values[[1]]
# rf
auc_rf = performance(pr_rf, measure = "auc")
auc_rf = auc_rf@y.values[[1]]
# svm
auc_svm = performance(pr_svm, measure = "auc")
auc_svm = auc_svm@y.values[[1]]
Methode <- c("glm","glm_tuned","knn_tuned","svm","svm_tuned","tree","tree_tuned","rf","rf_tuned")
Erreur <- c(round(err_glm*100,2),
round(err_glm_tuned*100,2),
round(err_knn_tuned*100,2),
round(err_svm*100,2),
round(err_svm_tuned*100,2),
round(err_tree*100,2),
round(err_tree_tuned*100,2),
round(err_rf*100,2),
round(err_rf_tuned*100,2)
)
Kappa <- c(round(kappa_glm,2),
round(kappa_glm_tuned,2),
round(kappa_knn_tuned,2),
round(kappa_svm,2),
round(kappa_svm_tuned,2),
round(kappa_tree,2),
round(kappa_tree_tuned,2),
round(kappa_rf,2),
round(kappa_rf_tuned,2)
)
Sensibilité <- c(round(sensitivity_glm,2),
round(sensitivity_glm_tuned,2),
round(sensitivity_knn_tuned,2),
round(sensitivity_svm,2),
round(sensitivity_svm_tuned,2),
round(sensitivity_tree,2),
round(sensitivity_tree_tuned,2),
round(sensitivity_rf,2),
round(sensitivity_rf_tuned,2)
)
Specificité <- c(round(specificity_glm,2),
round(specificity_glm_tuned,2),
round(specificity_knn_tuned,2),
round(specificity_svm,2),
round(specificity_svm_tuned,2),
round(specificity_tree,2),
round(specificity_tree_tuned,2),
round(specificity_rf,2),
round(specificity_rf_tuned,2)
)
AUC <- c("x",
round(auc_glm,2),
round(auc_knn,2),
"x",
round(auc_svm,2),
"x",
round(auc_tree,2),
"x",
round(auc_rf,2)
)
modele_compare <- data.frame(Methode, Erreur,Kappa, Sensibilité, Specificité, AUC)
modele_compare
write.csv(modele_compare,"./modele_compare.csv", row.names = FALSE)
model_list <- list(logistic = model_glm_tuned,
knn=model_knn_tuned,
rf = model_rf_tuned,
svm = model_svm_tuned,
tree = model_tree_tuned)
results <- resamples(model_list)
summary(results)
bwplot(results)
bwplot(results, metric = "ROC")
bwplot(results, metric = "Spec")
bwplot(results, metric = "Sens")
set.seed(42)
# apprentissage et tuning
grid <- expand.grid(C=seq(0.01,1,0.01))
model_svm_tuned <- train(stroke ~., data = df_training_over,
method = "svmLinear",
metric = "ROC",
preProcess = c("scale", "center"),
tuneGrid= grid,
trControl = df_control)
# affichage du modele
c = model_svm_tuned$results$C[model_svm_tuned$results$ROC == max(model_svm_tuned$results$ROC)]
ggplot(model_svm_tuned)+geom_vline(xintercept = c,color ='red')
# prediction
prediction_svm_tuned <- predict(model_svm_tuned, df_testing_over)
# matrice de confusion
matconfus_svm_tuned <- table(prediction_svm_tuned, df_testing_over$stroke)
matconfus_svm_tuned
# calcul de l'erreur de prediction
err_svm_tuned <- 1-(sum(diag(matconfus_svm_tuned)) / sum(matconfus_svm_tuned))
err_svm_tuned*100
# kappa / sensibilité / specificté
kappa_svm_tuned <- CohenKappa(matconfus_svm_tuned)
kappa_svm_tuned
sensitivity_svm_tuned <- sensitivity(prediction_svm_tuned, df_testing_over$stroke)
sensitivity_svm_tuned
specificity_svm_tuned <- specificity(prediction_svm_tuned, df_testing_over$stroke)
specificity_svm_tuned
set.seed(42)
# apprentissage et tuning
grid <- expand.grid(C=seq(0.01,1,0.03))
model_svm_tuned <- train(stroke ~., data = df_training_over,
method = "svmLinear",
metric = "ROC",
preProcess = c("scale", "center"),
tuneGrid= grid,
trControl = df_control)
# affichage du modele
c = model_svm_tuned$results$C[model_svm_tuned$results$ROC == max(model_svm_tuned$results$ROC)]
ggplot(model_svm_tuned)+geom_vline(xintercept = c,color ='red')
# prediction
prediction_svm_tuned <- predict(model_svm_tuned, df_testing_over)
# matrice de confusion
matconfus_svm_tuned <- table(prediction_svm_tuned, df_testing_over$stroke)
matconfus_svm_tuned
# calcul de l'erreur de prediction
err_svm_tuned <- 1-(sum(diag(matconfus_svm_tuned)) / sum(matconfus_svm_tuned))
err_svm_tuned*100
# kappa / sensibilité / specificté
kappa_svm_tuned <- CohenKappa(matconfus_svm_tuned)
kappa_svm_tuned
sensitivity_svm_tuned <- sensitivity(prediction_svm_tuned, df_testing_over$stroke)
sensitivity_svm_tuned
specificity_svm_tuned <- specificity(prediction_svm_tuned, df_testing_over$stroke)
specificity_svm_tuned
